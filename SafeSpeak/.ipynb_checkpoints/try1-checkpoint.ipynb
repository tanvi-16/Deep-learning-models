{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "031b1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout, Attention\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "train_file_path = \"train.csv\"\n",
    "test_file_path = \"test.csv\"\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Select relevant columns and drop NaN values\n",
    "train_df = train_df[['cleaned_tweet', 'subtask_a']].dropna()\n",
    "test_df = test_df[['cleaned_tweet', 'subtask_a']].dropna()\n",
    "\n",
    "# Convert labels (OFF -> 1, NOT -> 0)\n",
    "train_df['label'] = train_df['subtask_a'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "test_df['label'] = test_df['subtask_a'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_df['cleaned_tweet'] = train_df['cleaned_tweet'].apply(preprocess_text)\n",
    "test_df['cleaned_tweet'] = test_df['cleaned_tweet'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['cleaned_tweet'])\n",
    "X_train = tokenizer.texts_to_sequences(train_df['cleaned_tweet'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['cleaned_tweet'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Define improved BiLSTM Model with Attention\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100  # Trainable Embedding layer\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=True),\n",
    "    SpatialDropout1D(0.3),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Bidirectional(LSTM(64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c30bbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "206/206 [==============================] - 141s 614ms/step - loss: 0.6483 - accuracy: 0.6362 - val_loss: 0.5545 - val_accuracy: 0.7638\n",
      "Epoch 2/50\n",
      "206/206 [==============================] - 133s 644ms/step - loss: 0.4948 - accuracy: 0.7879 - val_loss: 0.4819 - val_accuracy: 0.7815\n",
      "Epoch 3/50\n",
      "206/206 [==============================] - 136s 662ms/step - loss: 0.3943 - accuracy: 0.8380 - val_loss: 0.5079 - val_accuracy: 0.7702\n",
      "Epoch 4/50\n",
      "206/206 [==============================] - 131s 635ms/step - loss: 0.3327 - accuracy: 0.8700 - val_loss: 0.5438 - val_accuracy: 0.7772\n",
      "Epoch 5/50\n",
      "206/206 [==============================] - 131s 638ms/step - loss: 0.2811 - accuracy: 0.8946 - val_loss: 0.6310 - val_accuracy: 0.7536\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), class_weight=class_weight_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save(\"safespeakmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4619064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Safe\n"
     ]
    }
   ],
   "source": [
    "# Function to predict new text\n",
    "def predict_text(text):\n",
    "    text = preprocess_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    return \"Harmful\" if prediction >= 0.5 else \"Safe\"\n",
    "\n",
    "# Example prediction\n",
    "test_text = \"Courtney Quinn is a fashion and makeup blogger from NYC. She shares her colorful world in her blog called Color Me Courtney, where you can also find various makeup tutorials, lifestyle posts, and more\"\n",
    "print(f\"Prediction: {predict_text(test_text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
