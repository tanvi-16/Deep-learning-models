{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018ba12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "412/412 [==============================] - 67s 142ms/step - loss: 0.5641 - accuracy: 0.7221 - val_loss: 0.4524 - val_accuracy: 0.8314\n",
      "Epoch 2/10\n",
      "412/412 [==============================] - 63s 153ms/step - loss: 0.3918 - accuracy: 0.8356 - val_loss: 0.4686 - val_accuracy: 0.7953\n",
      "Epoch 3/10\n",
      "412/412 [==============================] - 63s 154ms/step - loss: 0.2774 - accuracy: 0.8910 - val_loss: 0.5973 - val_accuracy: 0.7593\n",
      "Epoch 4/10\n",
      "412/412 [==============================] - 64s 155ms/step - loss: 0.2021 - accuracy: 0.9256 - val_loss: 0.7752 - val_accuracy: 0.7465\n",
      "Epoch 5/10\n",
      "412/412 [==============================] - 65s 158ms/step - loss: 0.1455 - accuracy: 0.9475 - val_loss: 0.8938 - val_accuracy: 0.7384\n",
      "Epoch 6/10\n",
      "412/412 [==============================] - 64s 157ms/step - loss: 0.1125 - accuracy: 0.9606 - val_loss: 1.1140 - val_accuracy: 0.7407\n",
      "Epoch 7/10\n",
      "412/412 [==============================] - 65s 158ms/step - loss: 0.0879 - accuracy: 0.9676 - val_loss: 1.1147 - val_accuracy: 0.7360\n",
      "Epoch 8/10\n",
      "412/412 [==============================] - 64s 155ms/step - loss: 0.0678 - accuracy: 0.9743 - val_loss: 1.2268 - val_accuracy: 0.7395\n",
      "Epoch 9/10\n",
      "412/412 [==============================] - 65s 159ms/step - loss: 0.0605 - accuracy: 0.9762 - val_loss: 1.4065 - val_accuracy: 0.7326\n",
      "Epoch 10/10\n",
      "412/412 [==============================] - 64s 156ms/step - loss: 0.0551 - accuracy: 0.9787 - val_loss: 1.6201 - val_accuracy: 0.7314\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset \n",
    "train_file_path = \"train.csv\"\n",
    "test_file_path = \"test.csv\"\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Select relevant columns and drop NaN values\n",
    "train_df = train_df[['cleaned_tweet', 'subtask_a']].dropna()\n",
    "test_df = test_df[['cleaned_tweet', 'subtask_a']].dropna()\n",
    "\n",
    "# Convert labels (OFF -> 1, NOT -> 0)\n",
    "train_df['label'] = train_df['subtask_a'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "test_df['label'] = test_df['subtask_a'].apply(lambda x: 1 if x == 'OFF' else 0)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['cleaned_tweet'])\n",
    "X_train = tokenizer.texts_to_sequences(train_df['cleaned_tweet'])\n",
    "X_test = tokenizer.texts_to_sequences(test_df['cleaned_tweet'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_df['label'])\n",
    "y_test = np.array(test_df['label'])\n",
    "\n",
    "# Define improved BiLSTM Model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100  # Trainable Embedding layer\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_length, trainable=True),\n",
    "    SpatialDropout1D(0.3),\n",
    "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save(\"safespeak_model.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c235c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Harmful\n"
     ]
    }
   ],
   "source": [
    "# Function to predict new text\n",
    "def predict_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    return \"Harmful\" if prediction >= 0.5 else \"Safe\"\n",
    "\n",
    "# Example prediction\n",
    "test_text = \"You are black and ugly. Move out\"\n",
    "print(f\"Prediction: {predict_text(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bd7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
